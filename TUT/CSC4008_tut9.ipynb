{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# CSC4008-tut9\n",
        "## K-Means & PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Review of K-Means##"
      ],
      "metadata": {
        "id": "4EYjkRgOyl58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans is an unsupervised machine learning algorithm that is commonly used for data clustering. The goal of the algorithm is to partition a dataset into a specified number of clusters $k$. The process can be divided into five steps.\n",
        "\n",
        "* Initialize: Choose the number of clusters $k$ and randomly initialize $k$ cluster centroids in the feature space.\n",
        "\n",
        "* Assign Data Points to Clusters: For each data point, calculate the distance between the data point and each of the $k$ centroids. Assign the data point to the cluster associated with the nearest centroid.\n",
        "\n",
        "* Update Centroids: After all data points have been assigned to clusters, calculate the mean of all data points within each cluster to obtain a new centroid for each cluster.\n",
        "\n",
        "* Repeat: Repeat the above two steps until convergence, which is typically determined by a maximum number of iterations or when the centroids no longer move significantly.\n",
        "\n",
        "* Output: The final centroids represent the k clusters, and the data points are classified into their corresponding clusters.\n",
        "\n",
        "**The initial placement of centroids** and **the number of cluster k** have large effects on the result of the algorithm. We can use **silhouette score** to measure the result. "
      ],
      "metadata": {
        "id": "CF6S9XNlyycm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mathematical insight of K-Means##"
      ],
      "metadata": {
        "id": "T3BiSXJD2uxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of K-Means is actually an optimization. Given the data set $\\{x_i\\}_{i=1}^n$, K-means aims to find cluster $c = \\{c_j\\}_{j=1}^k$ and assignment $r$, by minimizing the within-cluster variance, which is the sum of squared distances of data points to their assigned cluster centers.\n",
        "$$\\mathop{\\min}_{c,r} \\sum_{i}^{n} \\sum_{j}^{k} r_{ij}(x_i-c_j)^2$$\n",
        "$$s.t. \\quad r \\in \\{0,1\\}^{n \\times k}, \\quad \\sum_{j}^{k} r_{ij}=1$$\n",
        "Here $r_{ij}=1$ denotes $x_i$ is assigned to cluster $j$  \n",
        "To optimize such a problem, the major idea is to update $c$ and $r$ alternatively:\n",
        "* Given the cluster centers $c$, update the assignments $r$\n",
        "* Given the assignments $r$, update the cluster centers $c$  \n",
        "\n",
        "These two steps corresponding to the second and third step of K-Means.\n",
        "* Assignments: Given the cluster centers $c$, update the assignments $r$ by\n",
        "solving the following sub-problem\n",
        "$$\\mathop{\\min}_{r} \\sum_{i}^{n} \\sum_{j}^{k} r_{ij}(x_i-c_j)^2$$\n",
        "$$s.t. \\quad r \\in \\{0,1\\}^{n \\times k}, \\quad \\sum_{j}^{k} r_{ij}=1$$  \n",
        "Note that the assignment for each data $x_i$ can be solved independently. So we can further split the sub-problem into $n$ optimization problems.  \n",
        "$$\\mathop{\\min}_{r_i} \\sum_{j}^{k} r_{ij}(x_i-c_j)^2$$\n",
        "$$s.t. \\quad r_i = [0,...,1,...,0]$$\n",
        "It is easy to know that assign $x_i$ to the closest cluster is the optimal solution.  \n",
        "* Updating centroids: Given the assignments r, update the cluster centers c:\n",
        "$$\\mathop{\\min}_{c} \\sum_{i}^{n} \\sum_{j}^{k} r_{ij}(x_i-c_j)^2$$\n",
        "Note that $c_j$ can be optimized independently. So we can further split the sub-problem into $k$ optimization problems.  \n",
        "$$\\mathop{\\min}_{c_j} \\sum_{i}^{n} r_{ij}(x_i-c_j)^2$$\n",
        "The problem can be characterized by finding the cluster center given the coordinates of points in that cluster. So we can just set the derivative of the objective function to 0 to find the optimal solution.\n",
        "$$\\sum_{i}^{n} -2r_{ij}(x_i-c_j) = 0$$\n",
        "reformulate the equation, we have:\n",
        "$$c_j = \\frac{\\sum_{i}^{n} r_{ij}x_i}{\\sum_{i}^{n} r_{ij}}$$\n",
        "The resulting $c_j$ is definitely the mean of points in that cluster."
      ],
      "metadata": {
        "id": "mMVFNzlJ2zdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Silhouette score##"
      ],
      "metadata": {
        "id": "bVHocauB2UBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a clustering, for a single sample i, we define:\n",
        "* a(i): The mean distance between the sample and all other points in the same\n",
        "cluster.\n",
        "* b(i): The mean distance between the sample and all other points in the next\n",
        "nearest cluster.  \n",
        "\n",
        "Silhouette coefficient $s$ for sample i is formulated as:\n",
        "$$s(i) = \\frac{b(i) - a(i)}{max\\{a(i),b(i)\\}}$$\n",
        "Silhouette coefficient $s$ for a set of samples is given as the mean of the\n",
        "Silhouette coefficient for each sample."
      ],
      "metadata": {
        "id": "K1jdIPdp2W7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PCA##"
      ],
      "metadata": {
        "id": "zxCXy4-FFDwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA is a dimensionality reduction technique commonly used in ML to transform high-dimensional data into a lower-dimensional representation while retaining the information of original data as much as possible. \n",
        "\n",
        "* Data Preparation: Prepare the data by organizing it into a matrix where rows represent data points and columns represent features.\n",
        "\n",
        "* Data Standardization: Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
        "\n",
        "* Covariance Matrix Calculation: Compute the covariance matrix of the standardized data, which is a symmetric matrix that represents the pairwise covariance between all pairs of features.\n",
        "\n",
        "* Eigendecomposition: Perform eigendecomposition on the covariance matrix to obtain its eigenvalues and eigenvectors. \n",
        "\n",
        "* Principal Component Selection: Sort the eigenvalues in descending order and select the top $k$ eigenvectors (principal components) that desired.\n",
        "\n",
        "* Projection: Project the original data onto the selected principal components by taking the dot product between the standardized data and the selected eigenvectors. This results in a new lower-dimensional representation of the data, where the number of dimensions is reduced from the original number of features to $k$."
      ],
      "metadata": {
        "id": "jUCWdl86FJFj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's set up Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d35bc71c-7487-4ccb-b452-3620e74642b0"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=fa943fd98eeba875894535912459d906232a67e78e515709d9b5373ecfb70c62\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 122349 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u362-ga-0ubuntu1~20.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u362-ga-0ubuntu1~20.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "Now we import some of the libraries usually needed by our workload.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twk-K-jilWK7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtrJlMBt1Ela"
      },
      "source": [
        "Let's initialize the Spark context."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "KH91tEik9zZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYRX2PMm0L6"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hXdMR6wnEIM"
      },
      "source": [
        "In this Colab, rather than downloading a file from Google Drive, we will load a famous machine learning dataset, the [Breast Cancer Wisconsin dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), using the ```scikit-learn``` datasets loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K93ABEy9Zlo"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast_cancer = load_breast_cancer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpsaYOqRxar2"
      },
      "source": [
        "For convenience, given that the dataset is small, we first \n",
        "\n",
        "*   construct a Pandas dataframe\n",
        "*   tune the schema\n",
        "*   and convert it into a Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oitav_xhQD9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e64a22-e8c5-40ba-acb4-34e524ec0b55"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "pd_df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "df = spark.createDataFrame(pd_df)\n",
        "\n",
        "def set_df_columns_nullable(spark, df, column_list, nullable=False):\n",
        "  for struct_field in df.schema:\n",
        "    if struct_field.name in column_list:\n",
        "      struct_field.nullable = nullable\n",
        "  df_mod = spark.createDataFrame(df.rdd, df.schema)\n",
        "  return df_mod\n",
        "\n",
        "df = set_df_columns_nullable(spark, df, df.columns)\n",
        "df = df.withColumn('features', array(df.columns))\n",
        "vectors = df.rdd.map(lambda row: Vectors.dense(row.features))\n",
        "\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mean radius: double (nullable = false)\n",
            " |-- mean texture: double (nullable = false)\n",
            " |-- mean perimeter: double (nullable = false)\n",
            " |-- mean area: double (nullable = false)\n",
            " |-- mean smoothness: double (nullable = false)\n",
            " |-- mean compactness: double (nullable = false)\n",
            " |-- mean concavity: double (nullable = false)\n",
            " |-- mean concave points: double (nullable = false)\n",
            " |-- mean symmetry: double (nullable = false)\n",
            " |-- mean fractal dimension: double (nullable = false)\n",
            " |-- radius error: double (nullable = false)\n",
            " |-- texture error: double (nullable = false)\n",
            " |-- perimeter error: double (nullable = false)\n",
            " |-- area error: double (nullable = false)\n",
            " |-- smoothness error: double (nullable = false)\n",
            " |-- compactness error: double (nullable = false)\n",
            " |-- concavity error: double (nullable = false)\n",
            " |-- concave points error: double (nullable = false)\n",
            " |-- symmetry error: double (nullable = false)\n",
            " |-- fractal dimension error: double (nullable = false)\n",
            " |-- worst radius: double (nullable = false)\n",
            " |-- worst texture: double (nullable = false)\n",
            " |-- worst perimeter: double (nullable = false)\n",
            " |-- worst area: double (nullable = false)\n",
            " |-- worst smoothness: double (nullable = false)\n",
            " |-- worst compactness: double (nullable = false)\n",
            " |-- worst concavity: double (nullable = false)\n",
            " |-- worst concave points: double (nullable = false)\n",
            " |-- worst symmetry: double (nullable = false)\n",
            " |-- worst fractal dimension: double (nullable = false)\n",
            " |-- features: array (nullable = false)\n",
            " |    |-- element: double (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtR1xRvonxiO"
      },
      "source": [
        "With the next cell, we build the two data structures that we will be using throughout this Colab:\n",
        "\n",
        "\n",
        "*   ```features```, a dataframe of Dense vectors, containing all the original features in the dataset;\n",
        "*   ```labels```, a series of binary labels indicating if the corresponding set of features belongs to a subject with breast cancer, or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP23Xkgwi0SD"
      },
      "source": [
        "features = spark.createDataFrame(vectors.map(Row), [\"features\"])\n",
        "labels = np.array(breast_cancer.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7"
      },
      "source": [
        "### Your task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLNUxP0_8x3"
      },
      "source": [
        "You are now ready to cluster the data with the [K-means](https://spark.apache.org/docs/latest/ml-clustering.html) algorithm included in MLlib (Spark's Machine Learning library).\n",
        "Set the ```k``` parameter to **2**, fit the model, and the compute the [Silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)) (i.e., a measure of quality of the obtained clustering).  \n",
        "\n",
        "**IMPORTANT:** use the MLlib implementation of the Silhouette score (via ```ClusteringEvaluator```)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xVIfPHZwWaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c14d39-af58-4a86-b28b-19e1c6e39bc6"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "def k_means(feature):\n",
        "  #train a k-means model\n",
        "  kmeans = KMeans().setK(2).setSeed(1)\n",
        "  model = kmeans.fit(feature)\n",
        "  #make predictions\n",
        "  predictions = model.transform(feature)\n",
        "  #evaluate clustering by computing Silhouette score\n",
        "  evaluator = ClusteringEvaluator()\n",
        "  silhouette = evaluator.evaluate(predictions)\n",
        "  predictions.show(10)\n",
        "  print(\"Silhouette with squared euclidean distance =\", str(silhouette))\n",
        "  return predictions\n",
        "\n",
        "predictions = k_means(features)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|            features|prediction|\n",
            "+--------------------+----------+\n",
            "|[17.99,10.38,122....|         1|\n",
            "|[20.57,17.77,132....|         1|\n",
            "|[19.69,21.25,130....|         1|\n",
            "|[11.42,20.38,77.5...|         0|\n",
            "|[20.29,14.34,135....|         1|\n",
            "|[12.45,15.7,82.57...|         0|\n",
            "|[18.25,19.98,119....|         1|\n",
            "|[13.71,20.83,90.2...|         0|\n",
            "|[13.0,21.82,87.5,...|         0|\n",
            "|[12.46,24.04,83.9...|         0|\n",
            "+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Silhouette with squared euclidean distance = 0.8342904262826145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GB09n7sqTO6"
      },
      "source": [
        "Take the predictions produced by K-means, and compare them with the ```labels``` variable (i.e., the ground truth from our dataset).  \n",
        "\n",
        "Compute how many data points in the dataset have been clustered correctly (i.e., positive cases in one cluster, negative cases in the other).\n",
        "\n",
        "**HINT**: you can use ```np.count_nonzero(series_a == series_b)``` to quickly compute the element-wise comparison of two arrays.\n",
        "\n",
        "**IMPORTANT**: K-means is a clustering algorithm, so it will not output a label for each data point, but just a cluster identifier!  As such, label ```0``` does not necessarily match the cluster identifier ```0```.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQhC3APIPPM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed67186-7c65-4834-aca6-b603675344f0"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def calculate_TP(prediction,label,pca=False):\n",
        "  predict_lst = prediction.toPandas()['prediction'].values\n",
        "  n_correct_1 = np.count_nonzero(predict_lst == label)\n",
        "  n_correct_2 = np.count_nonzero(1-predict_lst == label)\n",
        "  n_correct = np.max([n_correct_1,n_correct_2])\n",
        "  if pca:\n",
        "    print(\"Number of data points that have been clustered correctly after PCA:\",n_correct)\n",
        "    print(\"Percent of data points that have been clustered correctly after PCA:\",n_correct/len(label))\n",
        "  else:\n",
        "    print(\"Number of data points that have been clustered correctly:\",n_correct)\n",
        "    print(\"Percent of data points that have been clustered correctly:\",n_correct/len(label))\n",
        "\n",
        "calculate_TP(predictions,labels)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points that have been clustered correctly: 486\n",
            "Percent of data points that have been clustered correctly: 0.8541300527240774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLIprM1JsdTU"
      },
      "source": [
        "Now perform dimensionality reduction on the ```features``` using the [PCA](https://spark.apache.org/docs/latest/ml-features.html#pca) statistical procedure, available as well in MLlib.\n",
        "\n",
        "Set the ```k``` parameter to **2**, effectively reducing the dataset size of a **15X** factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4J8JMDkSb24"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "from pyspark.ml.feature import PCA\n",
        "pca = PCA(k=2,inputCol='features')\n",
        "pca_model = pca.fit(features)\n",
        "pca_model.setOutputCol('output')\n",
        "pca_df = pca_model.transform(features)\n",
        "pca_vector = pca_df.rdd.map(lambda row:row.output)\n",
        "pcaFeatures = spark.createDataFrame(pca_vector.map(Row),['features'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8leQR4-atMAl"
      },
      "source": [
        "Now run K-means with the same parameters as above, but on the ```pcaFeatures``` produced by the PCA reduction you just executed.\n",
        "\n",
        "Compute the Silhouette score, as well as the number of data points that have been clustered correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_snSSj5k2y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dae0dc8-14e7-4096-c7d7-6666459d1fd5"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "predictions_pca = k_means(pcaFeatures)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|            features|prediction|\n",
            "+--------------------+----------+\n",
            "|[-2260.0138862925...|         1|\n",
            "|[-2368.9937557820...|         1|\n",
            "|[-2095.6652015478...|         1|\n",
            "|[-692.69051005705...|         0|\n",
            "|[-2030.2124927427...|         1|\n",
            "|[-888.28005357607...|         0|\n",
            "|[-1921.0822124748...|         1|\n",
            "|[-1074.7813350047...|         0|\n",
            "|[-908.57847816188...|         0|\n",
            "|[-861.57844940756...|         0|\n",
            "+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Silhouette with squared euclidean distance = 0.8348610363444836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMUb_afmlIEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf43a36-eff7-4cde-ced3-ce6ab4e3ec4f"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "calculate_TP(predictions_pca,labels,pca=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points that have been clustered correctly after PCA: 486\n",
            "Percent of data points that have been clustered correctly after PCA: 0.8541300527240774\n"
          ]
        }
      ]
    }
  ]
}